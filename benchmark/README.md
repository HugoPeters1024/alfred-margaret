# benchmark

This is the code used to benchmark `alfred-margaret`.
The results of this benchmark are shown in this [Channable tech blog article](https://www.channable.com/tech/how-we-made-haskell-search-strings-as-fast-as-rust).

## Setup

We use Nix to manage dependencies such as `cargo`, `bazel`, `stack` and the required python packages.
In order to drop into a environment containing these, run:

```
# In the repository root (alfred-margaret folder)
nix run --arg benchTools true -c $SHELL
```

The first time you do this will likely take a while unless you've already downloaded those packages (some 5 GiB).

## Running the Benchmarks

Files in the `data` directory must follow this format:

```
Lorem
sunt
officia

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
```

The first part of this file defines the search terms (the "needles") `Lorem`, `sunt` and `officia`.
The second part, after a blank line, defines the corpus to search in  (the "haystack").
Currently, these files **must be encoded as UTF-16 Little-Endian without BOM**.

Each run of `benchmark.py` generates a pair of `.results` and `.stats` files.

### Python

Run the benchmark in this directory:

```
./benchmark.py ./naive.py --prefix python
```

### Rust

Compile the Rust implementation by running Cargo in the `rust` directory:

```
cargo build --release
```

Run the benchmark using the compiled binary:

```
./benchmark.py rust/target/release/acbench-rust --prefix rust
```

### Java

Compile the Java implementation by running Bazel in the `java` directory:

```
bazel build :acbench_deploy.jar
```

Run the benchmark using the script generated by Bazel:

```
./benchmark.py java/bazel-bin/acbench --prefix java
```

### Haskell

Compile the Haskell implementation by running Stack in the `haskell` directory:

```
stack build
```

Run the benchmark using the compiled binary:

```
./benchmark.py haskell/.stack-work/path/to/ac-bench --prefix haskell --data-directory data-utf8
```

You can figure out `path/to/ac-bench` by running `stack run --verbose ac-bench`.

Note that you must first convert the data files for the benchmark into UTF-8 and put them in `data-utf8`.
You can use `iconv` for this:

```
mkdir data-utf8
for f in $(ls data); do echo $f; iconv -f UTF-16LE -t UTF-8 data/$f -o data-utf8/$f; done
```

### Haskell, with Rust FFI

Compile the static `libacbench` library using `cargo` in the `rust-ffi/libacbench` directory:

```
cargo build --release
```

Make sure to pass `--release`! Now run Stack in the `rust-ffi` directory:

```
stack build
```

Run the benchmark using the compiled binary:

```
./benchmark.py rust-ffi/.stack-work/path/to/ac-bench --prefix rust-ffi --data-directory data-utf8
```

Since this version uses the UTF-8 as well, you have to generate the UTF-8 data first as described in the previous section.

## Inspecting the Results

Once you have a bunch of `.stats` files, you can inspect the results using `report.py`:

```
./report.py haskell.stats haskell-utf8.stats rust.stats java.stats python.stats
# Will print something like (depending on your machine...)
haskell.stats:
  mean time: 3.529 ± 0.010 seconds
   min time: 3.431 seconds

haskell-utf8.stats:
  mean time: 3.526 ± 0.028 seconds
   min time: 3.392 seconds

rust.stats:
  mean time: 4.771 ± 0.013 seconds
   min time: 4.685 seconds

java.stats:
  mean time: 7.452 ± 0.090 seconds
   min time: 7.143 seconds

python.stats:
  mean time: 11.370 ± 0.057 seconds
   min time: 10.973 seconds
```
